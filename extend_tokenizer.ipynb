{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tokenizer_json = json.loads(original_tokenizer._tokenizer.to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama_tokenizer.json', 'w') as f:\n",
    "    json.dump(initial_tokenizer_json, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tok_vocab = json.loads(original_tokenizer._tokenizer.to_str())['model']['vocab']\n",
    "initial_tok_merges = json.loads(original_tokenizer._tokenizer.to_str())['model']['merges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llama_3_ext.merges\") as f:\n",
    "    new_merges = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "for m in new_merges:\n",
    "    if m.endswith(\"\\n\"):\n",
    "        merges.append(m[:-1])\n",
    "    else:\n",
    "        merges.append(m)\n",
    "\n",
    "del new_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llama_3_ext.vocab\") as f:\n",
    "    vocab = {k:v for k,v in sorted(json.load(f).items(), key=lambda x: x[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fixed = copy.deepcopy(vocab)\n",
    "for k, v in vocab.items():\n",
    "    if v >= 128000:\n",
    "        vocab_fixed[k] = v + 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tokenizer_json['model']['merges'] = merges\n",
    "initial_tokenizer_json['model']['vocab'] = vocab_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After change:\", len(initial_tokenizer_json['model']['vocab']))\n",
    "print(\"Before change:\", len(json.loads(original_tokenizer._tokenizer.to_str())['model']['vocab']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After change:\", len(initial_tokenizer_json['model']['merges']))\n",
    "print(\"Before change:\", len(json.loads(original_tokenizer._tokenizer.to_str())['model']['merges']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original tokenizer\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "tokenizer_json = original_tokenizer.backend_tokenizer.to_str()\n",
    "\n",
    "tokenizer = Tokenizer.from_str(tokenizer_json)\n",
    "\n",
    "added_tokens = {token: idx for token, idx in original_tokenizer.get_added_vocab().items()}\n",
    "combined_vocab = {**added_tokens, **vocab_fixed}\n",
    "\n",
    "bpe = BPE(combined_vocab, [(merge.split()[0], merge.split()[1]) for merge in merges])\n",
    "\n",
    "new_tokenizer = Tokenizer(bpe)\n",
    "\n",
    "if tokenizer.pre_tokenizer:\n",
    "    new_tokenizer.pre_tokenizer = tokenizer.pre_tokenizer\n",
    "\n",
    "if tokenizer.normalizer:\n",
    "    new_tokenizer.normalizer = tokenizer.normalizer\n",
    "\n",
    "if tokenizer.decoder:\n",
    "    new_tokenizer.decoder = tokenizer.decoder\n",
    "\n",
    "if tokenizer.post_processor:\n",
    "    new_tokenizer.post_processor = tokenizer.post_processor\n",
    "\n",
    "new_transformer_tokenizer = PreTrainedTokenizerFast(tokenizer_object=new_tokenizer)\n",
    "\n",
    "new_transformer_tokenizer.save_pretrained(\"/maybe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer.save_pretrained(\"/temp_tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/maybe/tokenizer.json\") as f:\n",
    "    tok_json = json.load(f)\n",
    "\n",
    "tok_json['pre_tokenizer'] = initial_tokenizer_json['pre_tokenizer']\n",
    "tok_json['normalizer'] = initial_tokenizer_json['normalizer']\n",
    "tok_json['decoder'] = initial_tokenizer_json['decoder']\n",
    "tok_json['post_processor'] = initial_tokenizer_json['post_processor']\n",
    "\n",
    "with open(\"/maybe/tokenizer.json\", 'w') as f:\n",
    "    json.dump(tok_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"/temp_tok/tokenizer_config.json\") as f:\n",
    "    tok_config_json = json.load(f)\n",
    "with open(\"/maybe/tokenizer_config.json\") as f:\n",
    "    json.dump(tok_config_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"/temp_tok/special_tokens_map.json\") as f:\n",
    "    tok_config_json = json.load(f)\n",
    "with open(\"/maybe/special_tokens_map.json\") as f:\n",
    "    json.dump(tok_config_json, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"rm -rf /temp_tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = AutoTokenizer.from_pretrained(\"/maybe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {v:k for k,v in tt.get_vocab().items()}\n",
    "itos[128000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_original = {v:k for k,v in tt.get_vocab().items()}\n",
    "itos_original[128000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.encode(\"<|begin_of_text|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer.encode(\"<|begin_of_text|>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
